{"cells":[{"cell_type":"markdown","metadata":{"id":"29c7oLlJbWwF"},"source":["# 570 Final Project - Parth R. Doshi"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1850,"status":"ok","timestamp":1700550518538,"user":{"displayName":"Parth Doshi","userId":"10612697865353633742"},"user_tz":300},"id":"ZVoOtpx3qocY","outputId":"cafaacb1-aa56-4081-9b22-70be4fcdf5ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"vJ60XfA-pwaF"},"source":["INSTALLING DEPENDENCIES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RAhT1XvkoKs"},"outputs":[],"source":["!pip install diffusers accelerate safetensors transformers\n","import PIL\n","import requests\n","import torch\n","import numpy as np\n","import tensorflow_datasets as tfds\n","from PIL import Image\n","from IPython import display\n","from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler"]},{"cell_type":"markdown","metadata":{"id":"yQe0anrgjiQN"},"source":["PRELIMINARY EXPERIMENTS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02dZwKl-jMKi"},"outputs":[],"source":["  #LOADING THE MODEL\n","  url = \"https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg\"\n","  def download_image(url):\n","      image = PIL.Image.open(requests.get(url, stream=True).raw)\n","      image = PIL.ImageOps.exif_transpose(image)\n","      image = image.convert(\"RGB\")\n","      return image\n","  image = download_image(url)\n","\n","  model_id = \"timbrooks/instruct-pix2pix\"\n","  instruction = \"make him into a cyborg\"\n","  pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n","  pipe.to(\"cuda\")\n","  pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7fBfTohpdOl"},"outputs":[],"source":[" #EXPERIMENT 1\n","instruction = \"make him into a blue clown\"\n","images = pipe(instruction, image=image, num_inference_steps=10, image_guidance_scale=1.0).images\n","print(f\"\\nInstructPix2Pix Output:\\n\")\n","images[0]"]},{"cell_type":"markdown","metadata":{"id":"dshZ93Bcp4un"},"source":["VARYING NUMBER OF INFERENCE STEPS PARAMETER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaGXANTlobHw"},"outputs":[],"source":["  #EXPERIMENT 2 - PART 1\n","  images = pipe(instruction, image=image, num_inference_steps=10, image_guidance_scale=1.0).images\n","  print(f\"\\nInstructPix2Pix Output:\\n\")\n","  images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-effrmuYokzy"},"outputs":[],"source":["  #EXPERIMENT 2 - PART 2\n","  images = pipe(instruction, image=image, num_inference_steps=100, image_guidance_scale=1.0).images\n","  print(f\"\\nIncreased number of inference steps (num_inference_steps = 10):\\n\")\n","  images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Wli1ugoom8h"},"outputs":[],"source":["  #EXPERIMENT 2 - PART 3\n","  images = pipe(instruction, image=image, num_inference_steps=1000, image_guidance_scale=1.0).images\n","  print(f\"\\nIncreased number of inference steps (num_inference_steps = 1000):\\n\")\n","  images[0]"]},{"cell_type":"markdown","metadata":{"id":"f5-wvYhGqG01"},"source":["VARYING IMAGE GUIDANCE SCALE PARAMETER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9otru8pXqMK1"},"outputs":[],"source":["  #EXPERIMENT 3 - PART 1\n","  instruction = \"make him into a doctor\"\n","  images = pipe(instruction, image=image, num_inference_steps=10, image_guidance_scale=1.0).images\n","  print(f\"\\nInstructPix2Pix Output:\\n\")\n","  images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6a4tYuEDqM1k"},"outputs":[],"source":["  #EXPERIMENT 3 - PART 2\n","  instruction = \"make him into a doctor\"\n","  images = pipe(instruction, image=image, num_inference_steps=10, image_guidance_scale=10.0).images\n","  print(f\"\\nInstructPix2Pix Output:\\n\")\n","  images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGle9Ll8qNVr"},"outputs":[],"source":["  #EXPERIMENT 3 - PART 3\n","  instruction = \"make him into a doctor\"\n","  images = pipe(instruction, image=image, num_inference_steps=10, image_guidance_scale=0.1).images\n","  print(f\"\\nInstructPix2Pix Output:\\n\")\n","  images[0]"]},{"cell_type":"markdown","metadata":{"id":"jWDvisSdjmN_"},"source":["EXTENDED EXPERIMENTS"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1700549838248,"user":{"displayName":"Parth Doshi","userId":"10612697865353633742"},"user_tz":300},"id":"l7OogZYi7qwT"},"outputs":[],"source":["#LOADING THE DATASETS\n","DATASETS = [\n","    \"fractal20220817_data\",\n","    \"kuka\",\n","    \"bridge\",\n","    \"taco_play\",\n","    \"jaco_play\",\n","    \"berkeley_cable_routing\",\n","    \"roboturk\",\n","    \"nyu_door_opening_surprising_effectiveness\",\n","    \"viola\",\n","    \"berkeley_autolab_ur5\",\n","    \"toto\",\n","    \"language_table\",\n","    \"columbia_cairlab_pusht_real\",\n","    \"stanford_kuka_multimodal_dataset_converted_externally_to_rlds\",\n","    \"nyu_rot_dataset_converted_externally_to_rlds\",\n","    \"stanford_hydra_dataset_converted_externally_to_rlds\",\n","    \"austin_buds_dataset_converted_externally_to_rlds\",\n","    \"nyu_franka_play_dataset_converted_externally_to_rlds\",\n","    \"maniskill_dataset_converted_externally_to_rlds\",\n","    \"cmu_franka_exploration_dataset_converted_externally_to_rlds\",\n","    \"ucsd_kitchen_dataset_converted_externally_to_rlds\",\n","    \"ucsd_pick_and_place_dataset_converted_externally_to_rlds\",\n","    \"austin_sailor_dataset_converted_externally_to_rlds\",\n","    \"austin_sirius_dataset_converted_externally_to_rlds\",\n","    \"bc_z\",\n","    \"usc_cloth_sim_converted_externally_to_rlds\",\n","    \"utokyo_pr2_opening_fridge_converted_externally_to_rlds\",\n","    \"utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds\",\n","    \"utokyo_saytap_converted_externally_to_rlds\",\n","    \"utokyo_xarm_pick_and_place_converted_externally_to_rlds\",\n","    \"utokyo_xarm_bimanual_converted_externally_to_rlds\",\n","    \"robo_net\",\n","    \"berkeley_mvp_converted_externally_to_rlds\",\n","    \"berkeley_rpt_converted_externally_to_rlds\",\n","    \"kaist_nonprehensile_converted_externally_to_rlds\",\n","    \"stanford_mask_vit_converted_externally_to_rlds\",\n","    \"tokyo_u_lsmo_converted_externally_to_rlds\",\n","    \"dlr_sara_pour_converted_externally_to_rlds\",\n","    \"dlr_sara_grid_clamp_converted_externally_to_rlds\",\n","    \"dlr_edan_shared_control_converted_externally_to_rlds\",\n","    \"asu_table_top_converted_externally_to_rlds\",\n","    \"stanford_robocook_converted_externally_to_rlds\",\n","    \"eth_agent_affordances\",\n","    \"imperialcollege_sawyer_wrist_cam\",\n","    \"iamlab_cmu_pickup_insert_converted_externally_to_rlds\",\n","    \"uiuc_d3field\",\n","    \"utaustin_mutex\",\n","    \"berkeley_fanuc_manipulation\",\n","    \"cmu_play_fusion\",\n","    \"cmu_stretch\",\n","    \"berkeley_gnm_recon\",\n","    \"berkeley_gnm_cory_hall\",\n","    \"berkeley_gnm_sac_son\",\n","]\n","\n","\n","def dataset2path(name):\n","    if name == \"robo_net\":\n","        version = \"1.0.0\"\n","    elif name == \"language_table\":\n","        version = \"0.0.1\"\n","    else:\n","        version = \"0.1.0\"\n","    return f\"gs://gresearch/robotics/{name}/{version}\"\n","\n","\n","def as_gif(images, path=\"temp.gif\"):\n","    # Render the images as the gif:\n","    images[0].save(path, save_all=True, append_images=images[1:], duration=1000, loop=0)\n","    gif_bytes = open(path, \"rb\").read()\n","    return gif_bytes\n","\n","\n","def mse_loss(first_image, last_image, ip2p_image):\n","    # Ensure the images have the same shape\n","    assert first_image.shape == last_image.shape == ip2p_image.shape, \"Input images must have the same shape\"\n","\n","    # Calculate MSE loss\n","    mse = np.mean((last_image - ip2p_image) ** 2)\n","\n","    return mse"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":1691,"status":"ok","timestamp":1700549850293,"user":{"displayName":"Parth Doshi","userId":"10612697865353633742"},"user_tz":300},"id":"Gcw4eHmxbZjx"},"outputs":[],"source":["# choose the dataset path in the dropdown on the right and rerun this cell\n","# to see multiple samples\n","\n","dataset = \"fractal20220817_data\"  # @param ['fractal20220817_data', 'kuka', 'bridge', 'taco_play', 'jaco_play', 'berkeley_cable_routing', 'roboturk', 'nyu_door_opening_surprising_effectiveness', 'viola', 'berkeley_autolab_ur5', 'toto', 'language_table', 'columbia_cairlab_pusht_real', 'stanford_kuka_multimodal_dataset_converted_externally_to_rlds', 'nyu_rot_dataset_converted_externally_to_rlds', 'stanford_hydra_dataset_converted_externally_to_rlds', 'austin_buds_dataset_converted_externally_to_rlds', 'nyu_franka_play_dataset_converted_externally_to_rlds', 'maniskill_dataset_converted_externally_to_rlds', 'furniture_bench_dataset_converted_externally_to_rlds', 'cmu_franka_exploration_dataset_converted_externally_to_rlds', 'ucsd_kitchen_dataset_converted_externally_to_rlds', 'ucsd_pick_and_place_dataset_converted_externally_to_rlds', 'austin_sailor_dataset_converted_externally_to_rlds', 'austin_sirius_dataset_converted_externally_to_rlds', 'bc_z', 'usc_cloth_sim_converted_externally_to_rlds', 'utokyo_pr2_opening_fridge_converted_externally_to_rlds', 'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds', 'utokyo_saytap_converted_externally_to_rlds', 'utokyo_xarm_pick_and_place_converted_externally_to_rlds', 'utokyo_xarm_bimanual_converted_externally_to_rlds', 'robo_net', 'berkeley_mvp_converted_externally_to_rlds', 'berkeley_rpt_converted_externally_to_rlds', 'kaist_nonprehensile_converted_externally_to_rlds', 'stanford_mask_vit_converted_externally_to_rlds', 'tokyo_u_lsmo_converted_externally_to_rlds', 'dlr_sara_pour_converted_externally_to_rlds', 'dlr_sara_grid_clamp_converted_externally_to_rlds', 'dlr_edan_shared_control_converted_externally_to_rlds', 'asu_table_top_converted_externally_to_rlds', 'stanford_robocook_converted_externally_to_rlds', 'eth_agent_affordances', 'imperialcollege_sawyer_wrist_cam', 'iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'uiuc_d3field', 'utaustin_mutex', 'berkeley_fanuc_manipulation', 'cmu_food_manipulation', 'cmu_play_fusion', 'cmu_stretch', 'berkeley_gnm_recon', 'berkeley_gnm_cory_hall', 'berkeley_gnm_sac_son']\n","display_key = \"image\"\n","\n","b = tfds.builder_from_directory(builder_dir=dataset2path(dataset))\n","if display_key not in b.info.features[\"steps\"][\"observation\"]:\n","    raise ValueError(\n","        f\"The key {display_key} was not found in this dataset.\\n\"\n","        + \"Please choose a different image key to display for this dataset.\\n\"\n","        + \"Here is the observation spec:\\n\"\n","        + str(b.info.features[\"steps\"][\"observation\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3k7E2Xy65Tps"},"outputs":[],"source":["ds = b.as_dataset(split=\"train[:10]\").shuffle(10)                # take only first 10 episodes\n","episodes = [ep for ep in list(ds)]\n","eps = [list(ep[\"steps\"].as_numpy_iterator()) for ep in episodes] # list of videos/episodes\n","my_dataset_dict = {'Initial image': [], 'Prompt': [], 'Expected Output': [], 'Output Image': []}\n","total_loss = 0\n","\n","for i in range(0, len(eps)):\n","  print(f'i: {i}')\n","  first_ep = eps[i]\n","  first_st = first_ep[0]                                                    # contains the first video, first frame\n","  first_obs = first_st[\"observation\"]                                       # contains the observation\n","  first_image = first_obs['image']\n","  instruction = first_obs['natural_language_instruction'].decode(\"utf-8\")   # gives the instruction\n","  last_st = first_ep[-1]                                                    # gives the last frame of the video\n","  last_image = last_st[\"observation\"][\"image\"]                              # gives the last image\n","\n","  # prompt: display first and last image\n","  display.display(Image.fromarray(first_image))\n","  print('\\n')\n","  print(f'Instruction : {instruction}')\n","  print('\\n')\n","  display.display(Image.fromarray(last_image))\n","\n","\n","  model_id = \"timbrooks/instruct-pix2pix\"\n","  pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n","  pipe.to(\"cuda\")\n","  pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n","\n","  images = pipe(instruction, image=first_image, num_inference_steps=100, image_guidance_scale=1.0).images\n","  images[0]\n","\n","  ip2p_image = np.array(images[0])\n","  display.display(Image.fromarray(ip2p_image))\n","\n","  my_dataset_dict['Initial image'].append(first_image)\n","  my_dataset_dict['Prompt'].append(instruction)\n","  my_dataset_dict['Expected Output'].append(last_image)\n","  my_dataset_dict['Output Image'].append(ip2p_image)\n","\n","  # Calculate MSE loss\n","  loss = mse_loss(first_image, last_image, ip2p_image)\n","\n","  print(f\"\\nMSE Loss: {loss}\\n\")\n","  total_loss += loss\n","\n","print(f\"\\nAverage MSE Loss: {total_loss/len(eps)}\\n\")\n","# print(my_dataset_dict)"]},{"cell_type":"markdown","metadata":{"id":"F11GkHZXrb6S"},"source":["TRAINING FUNCTION IMPLEMENTATION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGoO9BRyFpFB"},"outputs":[],"source":["# %cd /content/drive/MyDrive/Colab/instruct-pix2pix\n","# %yq eval-all -i 'select(fileIndex == 0) * select(fileIndex == 1)' configs/train.yaml script.yml\n","# %python main.py --name default --base configs/train.yaml --train\n","\n","# data:\n","#   target: main.DataModuleFromConfig\n","#   params:\n","#     batch_size: 32\n","#     num_workers: 2\n","#     train:\n","#       target: edit_dataset.EditDataset\n","#       params:\n","#         path: data/my_dict_dataset-dataset\n","#         split: train\n","#         min_resize_res: 256\n","#         max_resize_res: 256\n","#         crop_res: 256\n","#         flip_prob: 0.5\n","#     validation:\n","#       target: edit_dataset.EditDataset\n","#       params:\n","#         path: data/my_dict_dataset-dataset\n","#         split: val\n","#         min_resize_res: 256\n","#         max_resize_res: 256\n","#         crop_res: 256"]},{"cell_type":"markdown","metadata":{"id":"bGaTnuPmserM"},"source":["CUSTOM INPUT SELECTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZAQxxDTsThL"},"outputs":[],"source":["  # import cv2\n","  # # Read your images (replace these paths with image paths)\n","  # first_image = cv2.imread('path_to_first_image.jpg', cv2.IMREAD_COLOR)\n","  # last_image = cv2.imread('path_to_last_image.jpg', cv2.IMREAD_COLOR)\n","  # expected_image = cv2.imread('path_to_expected_image.jpg', cv2.IMREAD_COLOR)\n","\n","  # # Convert images to NumPy arrays\n","  # first_image = np.asarray(first_image)\n","  # last_image = np.asarray(last_image)\n","  # expected_image = np.asarray(expected_image)\n","\n","  # model_id = \"timbrooks/instruct-pix2pix\"\n","  # pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n","  # pipe.to(\"cuda\")\n","  # pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n","\n","  # images = pipe(instruction, image=first_image, num_inference_steps=100, image_guidance_scale=1.0).images\n","  # images[0]\n","  # ip2p_image = np.array(images[0])\n","  # display.display(Image.fromarray(ip2p_image))"]},{"cell_type":"markdown","metadata":{"id":"-dfdBeTEsbAo"},"source":["RERUN AFTER RETRAINING IP2P DIFFUSION MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tK_3oiXzrz7e"},"outputs":[],"source":["# ds = b.as_dataset(split=\"train[:10]\").shuffle(10)                # take only first 10 episodes\n","# episodes = [ep for ep in list(ds)]\n","# eps = [list(ep[\"steps\"].as_numpy_iterator()) for ep in episodes] # list of videos/episodes\n","# my_dataset_dict = {'Initial image': [], 'Prompt': [], 'Expected Output': [], 'Output Image': []}\n","# total_loss = 0\n","\n","# for i in range(0, len(eps)):\n","#   print(f'i: {i}')\n","#   first_ep = eps[i]\n","#   first_st = first_ep[0]                                                    # contains the first video, first frame\n","#   first_obs = first_st[\"observation\"]                                       # contains the observation\n","#   first_image = first_obs['image']\n","#   instruction = first_obs['natural_language_instruction'].decode(\"utf-8\")   # gives the instruction\n","#   last_st = first_ep[-1]                                                    # gives the last frame of the video\n","#   last_image = last_st[\"observation\"][\"image\"]                              # gives the last image\n","\n","#   # prompt: display first and last image\n","#   display.display(Image.fromarray(first_image))\n","#   print('\\n')\n","#   print(f'Instruction : {instruction}')\n","#   print('\\n')\n","#   display.display(Image.fromarray(last_image))\n","\n","#   model_id = \"timbrooks/instruct-pix2pix\"\n","#   pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n","#   pipe.to(\"cuda\")\n","#   pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n","\n","#   images = pipe(instruction, image=first_image, num_inference_steps=100, image_guidance_scale=1.0).images\n","#   images[0]\n","\n","#   ip2p_image = np.array(images[0])\n","#   display.display(Image.fromarray(ip2p_image))\n","\n","#   my_dataset_dict['Initial image'].append(first_image)\n","#   my_dataset_dict['Prompt'].append(instruction)\n","#   my_dataset_dict['Expected Output'].append(last_image)\n","#   my_dataset_dict['Output Image'].append(ip2p_image)\n","\n","\n","#   # Calculate MSE loss\n","#   loss = mse_loss(first_image, last_image, ip2p_image)\n","\n","#   print(f\"\\nMSE Loss: {loss}\\n\")\n","#   total_loss += loss\n","\n","# print(f\"\\nAverage MSE Loss: {total_loss/len(eps)}\\n\")\n","# # print(my_dataset_dict)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
